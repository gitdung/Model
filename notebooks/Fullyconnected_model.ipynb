{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZeLvsSJd0sC",
        "outputId": "2fd329f2-f238-4308-a6dd-885490e11133"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJJOiqildeVV",
        "outputId": "ccfbfa1e-0308-482b-81c8-6dbe1867fba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjuFHihZMehc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6TtEDbXMh_1"
      },
      "source": [
        "# Pre process input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bf1yRhu51c1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_3cPeLwM8kS"
      },
      "outputs": [],
      "source": [
        "def map_input_feature(batch):\n",
        "  return {\n",
        "      'input' : torch.tensor([ batch['entropy'] , batch['length'] , batch['num_Percent'] , batch['have_Special'] ])\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8S22c4nMrba",
        "outputId": "fa4d31f0-b132-48c2-8852-ec5326e2c966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['link', 'entropy', 'length', 'num_Percent', 'have_Special', 'label'],\n",
            "    num_rows: 14000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['link', 'entropy', 'length', 'num_Percent', 'have_Special', 'label'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['link', 'entropy', 'length', 'num_Percent', 'have_Special', 'label'],\n",
            "    num_rows: 3105\n",
            "})\n",
            "Dataset({\n",
            "    features: ['link', 'label', 'input'],\n",
            "    num_rows: 14000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['link', 'label', 'input'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['link', 'label', 'input'],\n",
            "    num_rows: 3105\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "train_data = load_dataset(\"csv\", data_files=\"/content/data_train_nn.csv\", sep=\",\" , names=[\"link\", \"entropy\", \"length\", \"num_Percent\", \"have_Special\", \"label\"] , split=\"train[0:14000]\")\n",
        "val_data = load_dataset(\"csv\", data_files=\"/content/data_train_nn.csv\", sep=\",\" , names=[\"link\", \"entropy\", \"length\", \"num_Percent\", \"have_Special\", \"label\"] , split=\"train[14000:]\")\n",
        "test_data = load_dataset(\"csv\", data_files=\"/content/data_test_nn.csv\", sep=\",\" , names=[\"link\", \"entropy\", \"length\", \"num_Percent\", \"have_Special\", \"label\"] , split=\"train[0:]\")\n",
        "\n",
        "print(train_data)\n",
        "print(val_data)\n",
        "print(test_data)\n",
        "\n",
        "# map input_feature gộp các đặc trưng lại thành 1 vector input  và xóa những cột ko cần thiết khi cho vào model\n",
        "train_data_pre = train_data.map(map_input_feature, remove_columns=['entropy', 'length', 'num_Percent', 'have_Special'])\n",
        "val_data_pre = val_data.map(map_input_feature, remove_columns=['entropy', 'length', 'num_Percent', 'have_Special'])\n",
        "test_data_pre = test_data.map(map_input_feature, remove_columns=['entropy', 'length', 'num_Percent', 'have_Special'])\n",
        "\n",
        "# chuyển cột label , input qua dạnh tensor\n",
        "train_data_pre.set_format(type=\"torch\", columns=['label' , 'input'])\n",
        "val_data_pre.set_format(type=\"torch\", columns=['label', 'input'])\n",
        "test_data_pre.set_format(type=\"torch\", columns=['label', 'input'])\n",
        "\n",
        "print(train_data_pre)\n",
        "print(val_data_pre)\n",
        "print(test_data_pre)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYQ47tGPEgV",
        "outputId": "1c35c31e-4af6-4e91-e73d-a4a73c12ed1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'label': tensor([1, 1, 1, 1, 1]),\n",
              " 'input': tensor([[ 3.0000,  8.0000,  0.0000,  1.0000],\n",
              "         [ 3.3083, 18.0000,  0.0000,  0.0000],\n",
              "         [ 2.6416,  9.0000, 22.2222,  0.0000],\n",
              "         [ 3.0958, 11.0000, 18.1818,  0.0000],\n",
              "         [ 1.9219,  5.0000,  0.0000,  0.0000]])}"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_pre[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHgttynpUirS"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Chuẩn bị model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5j0Um8PCzHz",
        "outputId": "a241b886-95e3-4fd4-f916-7f7454c2f3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "#mang no ron\n",
        "class NN(nn.Module):\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(NN, self).__init__()\n",
        "    # self.phoBert =\n",
        "    self.num_classes = num_classes\n",
        "    self.fc1 = nn.Linear(input_size, 256)\n",
        "    self.fc2 = nn.Linear(256, 256)\n",
        "    self.out = nn.Linear(256, num_classes)\n",
        "\n",
        "  def forward(self, input , labels):\n",
        "    x = F.relu(self.fc1(input.to(device)))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    logits = self.out(x)\n",
        "\n",
        "    # tính loss cái này chỉ để hiện kq loss tập valid\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      print(logits.shape)\n",
        "      print(labels.shape)\n",
        "      print(logits.view(-1, self.num_classes).shape)\n",
        "      print(labels.view(-1))\n",
        "      loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\n",
        "    return SequenceClassifierOutput(loss = loss , logits=logits) # hàm trainer cần cái này nó mới chịu train\n",
        "\n",
        "#thiet bị\n",
        "device = torch.device('cuda' if  torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "\n",
        "#sieu tham so\n",
        "input_size = 4 # fix 4\n",
        "num_classes = 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ads62uhNmWQP",
        "outputId": "d2679faf-aa35-4b4f-f043-17eb05b8b0f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NN(\n",
              "  (fc1): Linear(in_features=4, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tao mang no ron\n",
        "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqdxvKSUnJN6"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        " labels = pred.label_ids\n",
        " preds = pred.predictions.argmax(-1)\n",
        " f1 = f1_score(labels, preds, average=\"weighted\")\n",
        " acc = accuracy_score(labels, preds)\n",
        " return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2xTCT4KnUR2"
      },
      "outputs": [],
      "source": [
        "logging_steps = len(train_data) // batch_size\n",
        "model_name = f\"nn_URL_model\"\n",
        "training_args = TrainingArguments(output_dir=model_name, # đường dẫn model xuất ra\n",
        "                                  num_train_epochs=num_epochs, # số lần train\n",
        "                                  learning_rate=learning_rate, # LR\n",
        "                                  per_device_train_batch_size=batch_size, #batch_size\n",
        "                                  per_device_eval_batch_size=batch_size,\n",
        "                                  weight_decay=0.01,\n",
        "                                  evaluation_strategy=\"epoch\", # chiến lược đánh giá 2 option epoch vs step (nếu step thì bổ sung thêm evl_steps)\n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  eval_steps=40,\n",
        "                                  save_total_limit = 5, # save bn model thường thì nó sẽ giữa lại model có best model từ cao đến thâp\n",
        "                                  logging_dir='./logs',\n",
        "                                  log_level=\"error\",\n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  load_best_model_at_end=True, # load model tối nhất trong đống model đã lưu kia (bổ sung thêm metri_for_best_model)\n",
        "                                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0CvlMJYKiJf"
      },
      "outputs": [],
      "source": [
        "# class MyTrainer(Trainer):\n",
        "#   def compute_loss(self , model , inputs , return_outputs = False):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppHnYYB7n2Ga"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model, args=training_args,\n",
        "                  compute_metrics=compute_metrics,\n",
        "                  train_dataset=train_data_pre,\n",
        "                  eval_dataset=val_data_pre)\n",
        "trainer.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Z7uGaDWvu9"
      },
      "source": [
        "# Lưu model và đánh giá\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nT3JMcKtzr9"
      },
      "outputs": [],
      "source": [
        "#save model\n",
        "torch.save(model , 'model')\n",
        "the_model = torch.load('/content/drive/MyDrive/model_url_nn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at gechim/phobert-base-v2-finetuned and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0146, -0.0700, -0.1169,  ...,  0.1103,  0.1094,  0.1948],\n",
              "         [-0.1185,  0.1188, -0.4532,  ...,  0.0472,  0.3281,  0.2066],\n",
              "         [-0.1195,  0.0243, -0.5375,  ...,  0.4129,  0.2689,  0.3015],\n",
              "         ...,\n",
              "         [ 0.0432,  0.1896, -0.2310,  ..., -0.3814,  0.5556, -0.5428],\n",
              "         [ 0.0560,  0.0153, -0.0598,  ...,  0.3561,  0.2030, -0.0266],\n",
              "         [ 0.0307, -0.0421, -0.0879,  ...,  0.0820,  0.0647,  0.2323]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 1.8114e-01,  1.5267e-01,  2.6328e-01,  2.3842e-01,  8.7790e-02,\n",
              "         -1.5624e-01,  1.8803e-02,  4.6023e-02,  2.1273e-01, -8.7220e-02,\n",
              "          8.1869e-02,  1.8275e-01, -4.7798e-02,  3.2150e-02, -1.4706e-01,\n",
              "         -7.0745e-02, -2.4476e-01,  9.9409e-02,  1.0493e-01,  7.0500e-02,\n",
              "         -1.7365e-01, -2.4417e-02, -1.2071e-01, -7.3499e-02,  2.0065e-01,\n",
              "          7.9973e-02,  2.9614e-01, -3.0005e-01,  1.6484e-01,  2.8696e-03,\n",
              "         -2.3725e-01,  1.1971e-01,  2.2685e-02, -2.4012e-01, -3.3835e-02,\n",
              "         -1.0952e-01, -1.7355e-01,  4.2698e-03, -1.2750e-01,  4.1788e-02,\n",
              "         -2.0402e-01,  1.3715e-01,  7.3266e-02, -1.6235e-01,  4.4936e-02,\n",
              "         -2.4893e-01,  3.6126e-01,  1.6567e-01, -6.3221e-02, -6.6818e-02,\n",
              "         -1.7502e-01,  1.0842e-01,  7.7312e-02, -2.4403e-02,  1.1861e-01,\n",
              "         -5.9319e-02,  9.2557e-03, -1.6444e-01, -3.8398e-01, -8.1138e-02,\n",
              "         -2.0863e-01, -2.6866e-02,  1.4324e-01, -6.2579e-02,  1.5189e-01,\n",
              "         -7.1231e-02, -1.6837e-01,  1.9696e-01,  1.5986e-01,  4.3917e-02,\n",
              "         -4.9579e-02,  2.1206e-01, -1.1206e-01, -8.0976e-02,  4.5875e-02,\n",
              "         -1.2716e-02,  6.3862e-02, -1.8946e-01,  1.2523e-01, -3.0194e-02,\n",
              "         -8.9034e-02, -3.1871e-02,  2.2950e-01, -2.9724e-02,  1.4878e-01,\n",
              "         -1.8544e-01,  4.0102e-02, -1.4898e-01, -1.3070e-01, -1.9557e-01,\n",
              "         -5.8501e-02,  8.3697e-02, -9.9434e-02, -6.8448e-02, -4.5278e-02,\n",
              "          1.5251e-02, -9.6921e-02, -1.5819e-01, -7.1519e-02, -7.3031e-02,\n",
              "          1.3329e-01,  1.3473e-01,  5.9998e-02,  1.5399e-01,  1.3003e-01,\n",
              "         -2.4954e-01, -5.1628e-02,  8.6897e-02, -2.0106e-01,  2.7847e-01,\n",
              "         -2.7953e-02,  3.8316e-01, -6.2885e-02,  3.9205e-01, -9.7512e-02,\n",
              "         -1.8165e-01, -2.0526e-03,  2.4156e-02, -2.6112e-01,  9.1771e-02,\n",
              "          2.9236e-01,  3.1876e-02, -4.8972e-02,  2.0034e-02, -7.8376e-02,\n",
              "          1.6096e-01,  1.2492e-01, -6.8615e-02, -1.6858e-03,  1.9872e-01,\n",
              "         -1.7036e-02,  1.8567e-01, -1.5273e-01,  2.7285e-03,  1.0540e-04,\n",
              "         -1.7170e-01,  9.4887e-02,  2.4218e-01,  1.4708e-01, -1.6782e-01,\n",
              "         -8.8359e-02,  2.2350e-01,  6.6245e-02, -2.6100e-02, -1.3291e-01,\n",
              "         -6.3890e-02,  1.4867e-01, -6.4672e-02, -1.2169e-01, -1.1167e-01,\n",
              "         -1.3924e-01,  1.0404e-01,  1.5207e-01,  4.1937e-02,  1.0792e-02,\n",
              "         -9.7693e-02, -1.0951e-01, -7.3288e-02, -1.6356e-01,  2.3720e-01,\n",
              "         -5.2050e-02,  3.8854e-02, -1.5947e-01, -2.6211e-02, -1.5542e-02,\n",
              "          8.6774e-02,  3.3043e-01, -2.2946e-01,  8.6942e-02, -2.5101e-02,\n",
              "          1.0732e-01, -9.2408e-02, -1.2742e-01, -3.1971e-02, -1.3621e-01,\n",
              "         -1.4288e-01, -4.2843e-01,  1.6988e-01, -3.6584e-02,  8.6205e-02,\n",
              "         -3.7931e-02, -5.4138e-02, -7.7499e-02, -1.3178e-01,  1.9453e-01,\n",
              "          6.2073e-02,  1.7326e-01,  1.2641e-01, -3.4115e-02, -1.3297e-01,\n",
              "         -7.9292e-02, -1.4208e-01,  4.2982e-02, -2.1030e-01, -7.2082e-02,\n",
              "         -3.5001e-03,  1.3656e-01,  2.3034e-02, -2.3840e-01, -5.6134e-02,\n",
              "         -1.0451e-02,  2.9071e-01, -9.9421e-02, -9.6844e-02,  2.7507e-02,\n",
              "          7.1568e-02,  5.2721e-02,  7.4993e-02, -1.8082e-01, -5.1516e-02,\n",
              "         -6.6560e-02,  7.1899e-02,  1.3438e-02, -5.4650e-04, -2.4091e-02,\n",
              "          1.7533e-01, -1.2136e-01, -2.4848e-02, -8.1242e-02, -1.1570e-02,\n",
              "         -5.9587e-02, -4.1909e-02, -1.9134e-01,  6.5378e-02, -4.8665e-02,\n",
              "          3.8351e-02,  7.6655e-02, -1.5501e-03,  2.6538e-01, -3.4492e-03,\n",
              "         -8.9012e-03, -1.0422e-01, -6.9610e-02, -4.1459e-02,  9.4571e-02,\n",
              "          8.3950e-02,  4.4294e-03, -1.3256e-01, -1.2356e-01, -3.4745e-01,\n",
              "          1.3856e-01, -1.4201e-01,  1.4512e-02, -2.6292e-02,  2.3432e-01,\n",
              "         -9.9684e-02, -1.8534e-01,  6.7991e-02,  1.7300e-01,  6.0969e-02,\n",
              "         -6.0296e-04, -1.3168e-01,  5.4660e-02, -1.1018e-01,  3.5656e-02,\n",
              "          5.9708e-02,  9.3878e-02,  2.4847e-02,  1.7858e-01,  2.2619e-01,\n",
              "         -2.2254e-01,  1.1903e-01, -6.9500e-02,  3.1847e-01, -3.2535e-02,\n",
              "         -1.0583e-01,  4.3439e-02,  1.4653e-01, -7.6187e-02, -1.4056e-02,\n",
              "          1.1507e-02,  1.5865e-01,  5.6605e-02,  3.3258e-02, -5.3291e-02,\n",
              "         -6.8284e-03, -6.2802e-02, -6.8604e-02,  1.0853e-01,  7.4143e-03,\n",
              "         -3.2046e-01,  1.2844e-01, -2.9079e-01,  1.1172e-01, -1.8483e-01,\n",
              "          2.6964e-01,  3.4248e-02,  8.9859e-02,  3.8556e-01, -2.5557e-02,\n",
              "         -1.0529e-01,  1.3952e-02,  1.2489e-01, -3.0239e-01,  1.7538e-01,\n",
              "         -2.7185e-02,  1.1002e-01, -1.3805e-01,  2.6768e-01, -1.3342e-01,\n",
              "         -9.4086e-03, -2.0491e-01,  6.5026e-02,  1.1479e-01, -6.1703e-02,\n",
              "         -2.4518e-01,  5.0327e-02, -1.4702e-01,  1.6764e-01, -7.9157e-02,\n",
              "          6.8873e-02,  6.2687e-02, -2.0504e-01,  1.1302e-01,  1.0722e-02,\n",
              "          5.7320e-03, -7.6949e-02, -1.2781e-01, -1.1964e-01, -2.1811e-01,\n",
              "         -5.1404e-02, -9.8979e-03, -3.1990e-01, -1.0334e-01, -1.7234e-01,\n",
              "          3.8569e-02,  4.8577e-02,  1.2934e-01,  7.9831e-03,  7.2730e-03,\n",
              "         -1.4190e-01, -6.9937e-02,  8.9048e-02, -1.1989e-01,  1.6107e-01,\n",
              "          4.3246e-03, -2.2128e-01, -9.9906e-03, -2.6200e-01,  2.9396e-02,\n",
              "          8.3975e-02,  5.2112e-02,  2.5476e-01,  3.4904e-01, -1.3378e-01,\n",
              "         -3.5183e-01,  4.6471e-02,  2.2859e-01, -1.1157e-01, -7.1765e-02,\n",
              "          1.8983e-01,  1.5228e-02, -1.4445e-01,  3.0382e-02, -1.3771e-02,\n",
              "         -1.7104e-01, -1.8928e-01,  1.8797e-01,  1.5324e-02,  7.4626e-02,\n",
              "          2.5320e-01,  1.0490e-01,  1.2541e-01,  2.3379e-02,  8.9737e-02,\n",
              "         -8.0975e-02, -1.3347e-02,  8.9016e-02, -1.2725e-01,  1.1821e-01,\n",
              "          2.5719e-01, -2.1327e-01,  3.3622e-01,  4.3075e-03, -3.3726e-02,\n",
              "          7.0292e-02, -1.1338e-01,  1.6620e-01, -2.7457e-01,  2.9177e-01,\n",
              "         -1.7449e-01,  2.0637e-01, -1.0109e-04,  2.1684e-01,  2.2327e-01,\n",
              "         -4.7518e-02, -5.4758e-03,  3.2024e-01,  1.7602e-01,  1.1534e-01,\n",
              "          2.6735e-02,  7.5657e-02,  4.3822e-03,  3.0573e-01,  1.3080e-01,\n",
              "          5.5367e-03, -1.4422e-01, -2.4439e-01,  2.8872e-01,  2.6880e-01,\n",
              "          1.9567e-01, -8.8296e-02,  1.5987e-01,  9.8898e-02,  2.3492e-01,\n",
              "          2.2717e-01,  2.9457e-01,  1.5794e-01, -8.6029e-03, -2.1320e-02,\n",
              "         -1.8465e-01,  1.4552e-01,  2.3260e-02, -2.8723e-01, -2.3407e-02,\n",
              "         -2.0108e-01,  2.6058e-02,  3.7067e-02, -1.5590e-02,  3.2342e-01,\n",
              "         -2.1742e-01, -3.1322e-01, -1.1776e-01,  2.6193e-01, -1.2909e-02,\n",
              "          8.3106e-02, -8.8693e-02,  9.4258e-02,  6.6637e-02, -2.4837e-02,\n",
              "         -5.0674e-02, -1.1310e-01, -6.4843e-02,  1.8733e-01,  6.9407e-02,\n",
              "          5.2458e-03, -5.6577e-02,  1.0253e-01, -9.8231e-02, -1.0061e-01,\n",
              "          1.9280e-01, -1.8478e-01,  1.4585e-01, -8.5052e-02, -3.5279e-01,\n",
              "         -1.6465e-01,  4.5821e-02, -9.7071e-02,  1.0667e-01,  1.2224e-01,\n",
              "         -2.8417e-03, -2.0763e-01,  6.6215e-02,  7.5207e-02, -2.3160e-01,\n",
              "          1.7758e-01,  1.3033e-01, -2.9105e-02,  1.0722e-01,  6.6766e-02,\n",
              "         -2.1081e-01, -2.5767e-01,  1.8512e-01, -2.4876e-01, -1.5156e-01,\n",
              "         -7.7345e-02, -1.4041e-02,  1.1711e-01,  1.9197e-01,  5.2569e-02,\n",
              "         -1.6822e-01,  3.5907e-01, -1.1212e-01, -9.0725e-02,  1.5468e-01,\n",
              "         -3.4162e-02, -2.3276e-02, -8.3561e-02, -7.4845e-02,  8.7242e-02,\n",
              "         -1.4489e-01, -6.9293e-02, -1.8942e-01,  5.1840e-02, -1.3361e-01,\n",
              "         -6.4848e-02, -2.4255e-01,  2.0170e-01,  3.1161e-01, -9.5918e-02,\n",
              "          1.4703e-02, -6.8488e-02, -1.5565e-01, -2.2420e-01, -9.9332e-02,\n",
              "         -6.0085e-02,  9.4496e-03,  3.6108e-02,  3.4713e-02, -2.0966e-01,\n",
              "          6.7457e-03,  3.6444e-01,  1.6980e-02, -2.1141e-01,  1.9704e-01,\n",
              "          2.8029e-02, -3.3775e-02, -1.3665e-02, -1.8032e-01,  4.4705e-02,\n",
              "         -1.6971e-01,  3.1172e-01, -1.2604e-01,  2.2804e-01, -4.8859e-02,\n",
              "         -1.4073e-02, -1.0722e-01, -1.6004e-01,  2.4843e-02, -3.8347e-02,\n",
              "          2.8913e-01,  7.5508e-03,  1.0568e-01,  3.2673e-02,  2.1726e-01,\n",
              "         -1.6867e-01,  1.4862e-01, -5.8907e-02, -7.7350e-02,  1.4669e-01,\n",
              "          3.7676e-02, -1.7906e-01, -1.8196e-01,  2.2893e-02, -1.2176e-02,\n",
              "          2.4745e-01, -1.6981e-01, -4.5763e-02, -2.7574e-03, -3.1229e-02,\n",
              "          1.1302e-01,  1.1339e-01,  2.3678e-01, -1.0296e-01,  1.1068e-01,\n",
              "          1.3579e-01, -2.0683e-01,  3.7377e-02,  1.3081e-02, -1.6463e-02,\n",
              "         -6.8953e-02, -1.1953e-01, -1.1966e-01, -1.0187e-01,  5.8212e-02,\n",
              "          1.3857e-01,  7.5903e-02, -2.3368e-01, -9.8247e-02, -3.8922e-02,\n",
              "         -1.6321e-01,  9.5599e-02,  1.3154e-02,  1.9537e-01, -9.9737e-02,\n",
              "         -6.3076e-02,  4.5186e-02, -4.0715e-02, -7.0901e-02, -2.9557e-01,\n",
              "         -1.6034e-01, -2.3943e-02, -6.4591e-02, -1.0855e-01, -2.5909e-01,\n",
              "         -3.1968e-02,  5.5072e-02, -4.2669e-02,  1.1982e-01, -1.9638e-01,\n",
              "         -3.4762e-01, -1.3485e-01,  6.0200e-02,  1.1595e-01,  4.4661e-03,\n",
              "          1.1743e-01, -2.5615e-01,  3.1629e-02,  7.4136e-02, -1.0368e-01,\n",
              "          1.2327e-01, -2.4704e-01, -1.7359e-01,  1.5385e-01,  7.1377e-02,\n",
              "          2.7605e-01, -1.1766e-01,  1.8753e-01, -1.1297e-01, -2.5850e-01,\n",
              "         -3.6189e-02, -4.5783e-02, -9.9574e-03,  3.9692e-02, -1.1574e-01,\n",
              "          1.8341e-01,  1.1762e-01, -1.3429e-01, -1.5257e-01,  1.2031e-02,\n",
              "          2.1906e-02,  1.6045e-01, -1.0326e-02,  4.0246e-02,  9.5570e-02,\n",
              "         -4.2839e-02,  1.9400e-01,  3.2481e-02,  9.0451e-02,  7.5596e-02,\n",
              "         -2.2677e-02, -2.7423e-01, -4.1196e-02, -6.8890e-02, -2.4372e-01,\n",
              "          1.8607e-01,  1.7536e-02, -2.4056e-02, -1.7863e-02, -8.6510e-02,\n",
              "         -1.9262e-01,  9.8595e-03,  2.6581e-03,  1.3181e-01, -1.0486e-01,\n",
              "         -1.6342e-01,  1.3196e-01,  2.2895e-01, -2.7122e-02,  7.5202e-02,\n",
              "         -4.6656e-04, -4.0793e-02,  2.8094e-02,  1.3956e-01, -1.1555e-01,\n",
              "         -1.0339e-01, -1.5660e-01,  5.1190e-03, -2.4073e-01,  4.7686e-02,\n",
              "         -1.6284e-01, -9.1810e-02,  1.0961e-01, -3.9042e-01,  1.4589e-01,\n",
              "         -1.7718e-01, -1.4128e-04,  1.1090e-01, -2.3911e-01,  8.6713e-02,\n",
              "          1.5230e-01,  3.2126e-01, -4.7600e-02, -1.1134e-01,  1.7055e-01,\n",
              "          1.7520e-01, -7.0768e-02,  8.5564e-02, -6.0210e-02, -2.9752e-01,\n",
              "          2.8849e-01,  8.9469e-03, -1.1544e-01, -1.1824e-01, -7.4806e-02,\n",
              "          2.0655e-02, -1.1296e-01, -9.8425e-02,  3.2443e-01,  2.4452e-01,\n",
              "         -5.2201e-02,  1.4396e-01,  5.3654e-02, -2.3293e-01, -1.4408e-02,\n",
              "         -1.1058e-01,  1.1440e-01, -1.3780e-01, -1.1952e-01,  1.6427e-01,\n",
              "          1.0369e-01, -1.6708e-01,  1.2219e-01, -2.7701e-02,  3.2878e-01,\n",
              "         -2.1089e-01,  2.1791e-01, -7.2214e-02,  2.4800e-01, -1.5386e-01,\n",
              "         -1.1041e-01,  8.4788e-02, -1.7791e-02, -6.9271e-02,  2.4430e-01,\n",
              "         -3.0834e-02,  1.2962e-01,  6.6609e-02,  9.0497e-03,  4.6383e-03,\n",
              "          1.1629e-01,  2.0536e-01, -4.0029e-02,  1.9073e-01,  2.4992e-01,\n",
              "          8.2870e-02, -3.6535e-02,  8.9429e-02,  1.1033e-01, -1.0703e-01,\n",
              "          1.4355e-01,  2.1114e-02, -3.3502e-02, -1.4285e-01, -1.1864e-01,\n",
              "          1.0605e-01, -2.0411e-03,  1.0442e-01,  1.4402e-02, -1.0651e-01,\n",
              "         -3.8663e-02, -2.7577e-01,  2.9408e-02, -2.6039e-01,  1.8314e-01,\n",
              "          1.0659e-01, -4.6841e-02,  1.7620e-01, -1.4174e-01, -6.0874e-02,\n",
              "         -2.3931e-01, -6.0140e-02, -8.4384e-02, -2.1334e-01, -2.0493e-01,\n",
              "         -1.0611e-01, -4.6468e-02, -2.6972e-02, -3.8731e-02,  2.1419e-01,\n",
              "         -4.5794e-02, -4.2615e-02,  1.4467e-01, -2.4465e-01,  5.1603e-02,\n",
              "         -3.5777e-02,  1.6759e-01,  1.2792e-01,  1.6092e-01, -1.1554e-01,\n",
              "         -5.2746e-02, -1.5073e-01, -2.0243e-01, -9.0079e-02,  2.7706e-01,\n",
              "          3.9714e-03, -5.9019e-02, -3.8298e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, RobertaModel\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gechim/phobert-base-v2-finetuned\")\n",
        "model = RobertaModel.from_pretrained(\"gechim/phobert-base-v2-finetuned\")\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "outputs\n",
        "# last_hidden_states = outputs.last_hidden_sta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import RobertaConfig\n",
        "\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import  RobertaPreTrainedModel\n",
        "from transformers.models.roberta.modeling_roberta import  RobertaModel\n",
        "\n",
        "class RobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    config_class = RobertaConfig\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        \n",
        "        # định nghĩa các layer bên model của bảo\n",
        "        self.phoBertFineTunedURL = RobertaModel(config, add_pooling_layer=False) \n",
        "        \n",
        "\n",
        "        #định nghĩa các layer bên sang\n",
        "        \n",
        "        # định nghĩa các layer bên cls\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size) \n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        \n",
        "        # load pretrained model \n",
        "        self.init_weights()\n",
        "        \n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        \n",
        "        # output bên bảo  \n",
        "        # #output_phoBertFineTunedURL[0][:,0,:] \n",
        "        # (batch , 1 , 768)\n",
        "        output_phoBertFineTunedURL = self.phoBertFineTunedURL(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids) \n",
        "        \n",
        "        # out put bên sang \n",
        "        # .....\n",
        "        \n",
        "        #phần cls\n",
        "        dense_output =  self.dense(output_phoBertFineTunedURL[0][:,0,:]) # chổ nãy sẽ cộng lại nề\n",
        "        sequence_output = self.dropout(dense_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        \n",
        "        # Calculate losses\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            # Return model output object\n",
        "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoConfig , AutoModel , AutoModelForPreTraining\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "\n",
        "class PhoBERT_NN_Classifier(AutoModelForPreTraining):\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2label = {0: \"Bình thường\", 1: \"Bất thường\"}\n",
        "label2id = {\"Bình thường\": 0, \"Bất thường\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=64001, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoConfig\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "phobert_config = AutoConfig.from_pretrained('gechim/phobert-base-v2-finetuned', num_labels= 2, id2label=id2label, label2id=label2id)\n",
        "phobert_nn_cls_model = (PhoBERT_NN_Classifier.from_pretrained('vinai/phobert-base-v2', config=phobert_config).to(device))\n",
        "phobert_nn_cls_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"_name_or_path\": \"gechim/phobert-base-v2-finetuned\",\n",
              "  \"architectures\": [\n",
              "    \"RobertaForSequenceClassification\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"B\\u00ecnh th\\u01b0\\u1eddng\",\n",
              "    \"1\": \"B\\u1ea5t th\\u01b0\\u1eddng\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"B\\u00ecnh th\\u01b0\\u1eddng\": 0,\n",
              "    \"B\\u1ea5t th\\u01b0\\u1eddng\": 1\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 258,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"problem_type\": \"single_label_classification\",\n",
              "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.32.1\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 64001\n",
              "}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "phobert_model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"Hi there! My name is [Your Name], and I'm excited to be a part of this program today. Let me know if you have any questions or need help with anything.\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
