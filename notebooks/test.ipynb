{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at gechim/phobert-base-v2-finetuned and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded \n",
      "\n",
      "\n",
      "\n",
      "NN(\n",
      "  (phoBert): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=768, bias=True)\n",
      "  (dropout_nn): Dropout(p=0.1, inplace=False)\n",
      "  (dropout_lm): Dropout(p=0.1, inplace=False)\n",
      "  (out): Linear(in_features=1536, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'D:/Workspace/Project_VNNIC')\n",
    "from transformers import AutoTokenizer , AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from main.processing_data.getLexicalFeature import * \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gechim/phobert-base-v2-finetuned')\n",
    "phoBert = AutoModel.from_pretrained(\"gechim/phobert-base-v2-finetuned\")\n",
    "\n",
    "class NN(nn.Module):\n",
    "  def __init__(self, input_size, num_classes):\n",
    "    super(NN, self).__init__()\n",
    "    self.phoBert = phoBert # (batchsize , 1 , 768)\n",
    "    self.num_classes = num_classes\n",
    "    self.fc1 = nn.Linear(input_size, 256)\n",
    "    self.fc2 = nn.Linear(256, 768) #(batchsize , 1 , 768)\n",
    "    self.dropout_nn = nn.Dropout(0.1)\n",
    "    self.dropout_lm = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    # self.out = nn.Linear(768, num_classes)\n",
    "    self.out = nn.Linear(1536, num_classes)\n",
    "\n",
    "  def forward(self, features, input_ids, token_type_ids, attention_mask , labels):\n",
    "    # output bên sang\n",
    "    x_nn = F.relu(self.fc1(features))\n",
    "    x_nn = F.relu(self.fc2(x_nn))\n",
    "\n",
    "    # output bên bảo\n",
    "    x_phoBert = self.phoBert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).last_hidden_state[:,0,:]\n",
    "\n",
    "    #drop out trước khi concat\n",
    "    x_nn = self.dropout_nn(x_nn)\n",
    "    x_phoBert = self.dropout_lm(x_phoBert)\n",
    "\n",
    "    # print(x_phoBert.shape)\n",
    "    logits = self.out(torch.cat(( x_nn , x_phoBert) , dim=1)) #self.out( x_nn + x_phoBert)\n",
    "\n",
    "\n",
    "    # tính loss cái này chỉ để hiện kq loss tập valid\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      loss_fct = nn.CrossEntropyLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "    return SequenceClassifierOutput(loss = loss , logits=logits) # hàm trainer cần cái này nó mới chịu train\n",
    "\n",
    "print(\"Model loaded \\n\\n\\n\")\n",
    "model = torch.load('D:/Workspace/Project_VNNIC/models/model_concat_dataV2.pt' , map_location=torch.device('cpu'))\n",
    "print(model)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def predict(url):\n",
    "    print(getLexicalInputNN(url))\n",
    "    url = normalize_url(url)\n",
    "    url_tokenize = tokenizer(url , return_tensors='pt')\n",
    "    x_feature = torch.tensor([getLexicalInputNN(url)] , dtype=torch.float32)\n",
    "    y = model(features = x_feature,input_ids = url_tokenize['input_ids'] , token_type_ids = url_tokenize['token_type_ids'] , attention_mask = url_tokenize['attention_mask']  , labels = torch.tensor([1])).logits\n",
    "    if torch.argmax(y).item() == 0:\n",
    "        return \"Bình thường\"\n",
    "    if torch.argmax(y).item() == 1:\n",
    "        return \"Có tín nhiệm thấp\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dictionary: [Errno 2] No such file or directory: 'datas\\\\data_train_nn\\\\dict_check_type.xlsx'\n",
      "con_lai\n",
      "[3.    1.585 0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      "Error loading dictionary: [Errno 2] No such file or directory: 'datas\\\\data_train_nn\\\\dict_check_type.xlsx'\n",
      "con_lai\n",
      "Có tín nhiệm thấp\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"bet.com.vn\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
