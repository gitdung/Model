{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'D:/Workspace/Project_VNNIC')\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "# from main.processing_data.getLexicalFeature import * \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gechim/phobert-base-v2-finetuned')\n",
    "phoBert = AutoModel.from_pretrained(\"gechim/phobert-base-v2-finetuned\")\n",
    "\n",
    "#mang no ron\n",
    "\n",
    "class NN(nn.Module):\n",
    "  def __init__(self, input_size, num_classes):\n",
    "    super(NN, self).__init__()\n",
    "    self.phoBert = phoBert # (batchsize , 1 , 768)\n",
    "    self.num_classes = num_classes\n",
    "    self.fc1 = nn.Linear(input_size, 256)\n",
    "    self.fc2 = nn.Linear(256, 768) #(batchsize , 1 , 768)\n",
    "    self.dropout_nn = nn.Dropout(0.1)\n",
    "    self.dropout_lm = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    # self.out = nn.Linear(768, num_classes)\n",
    "    self.out = nn.Linear(1536, num_classes)\n",
    "\n",
    "  def forward(self, features, input_ids, token_type_ids, attention_mask , labels):\n",
    "    # output bên sang\n",
    "    x_nn = F.relu(self.fc1(features))\n",
    "    x_nn = F.relu(self.fc2(x_nn))\n",
    "\n",
    "    # output bên bảo\n",
    "    x_phoBert = self.phoBert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).last_hidden_state[:,0,:]\n",
    "\n",
    "    #drop out trước khi concat\n",
    "    x_nn = self.dropout_nn(x_nn)\n",
    "    x_phoBert = self.dropout_lm(x_phoBert)\n",
    "\n",
    "    # print(x_phoBert.shape)\n",
    "    logits = self.out(torch.cat(( x_nn , x_phoBert) , dim=1)) #self.out( x_nn + x_phoBert)\n",
    "\n",
    "\n",
    "    # tính loss cái này chỉ để hiện kq loss tập valid\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      loss_fct = nn.CrossEntropyLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "    return SequenceClassifierOutput(loss = loss , logits=logits) # hàm trainer cần cái này nó mới chịu train\n",
    "\n",
    "\n",
    "# def preprocess_url(url):\n",
    "#     url = str(url)\n",
    "#     if url.startswith(\"http://\"):\n",
    "#         url = url[7:]\n",
    "#         url = url.replace(\"www.\", \"\")\n",
    "#     if url.startswith(\"https://\"):\n",
    "#         url = url[8:]\n",
    "#         url = url.replace(\"www.\", \"\")\n",
    "#         url = url.replace(\".\", \" \")\n",
    "#     url = url.replace(\".\", \" \")\n",
    "#     url = url.replace(\"/\", \"\")\n",
    "#     return url\n",
    "\n",
    "\n",
    "def preprocess_url(url):\n",
    "    if url.startswith(\"http://\"):\n",
    "        url = url[7:]\n",
    "        url = url.replace(\"www.\", \"\")\n",
    "    if url.startswith(\"https://\"):\n",
    "        url = url[8:]\n",
    "        url = url.replace(\"www.\", \"\")\n",
    "        url = url.replace(\".\", \" \")\n",
    "    url = url.replace(\".\", \" \")\n",
    "    url = url.replace(\"/\", \"\")\n",
    "    url = url.replace(\"edu vn\", \"\")\n",
    "    url = url.replace(\"com vn\", \"\")\n",
    "    url = url.replace(\"net vn\", \"\")\n",
    "    url = url.replace(\"org vn\", \"\")\n",
    "    url = url.replace(\"gov vn\", \"\")\n",
    "    url = url.replace(\"vn\", \"\")\n",
    "    return url\n",
    "\n",
    "model = NN(10,2)\n",
    "model = torch.load('D:\\Workspace\\Project_VNNIC\\models\\model_concat_dataV2.pt' , map_location=torch.device('cpu'))\n",
    "    \n",
    "def detect_toxic_website(url):\n",
    "    # print(getLexicalInputNN(url))\n",
    "    url = preprocess_url(url)\n",
    "    url_tokenize = tokenizer(url , return_tensors='pt')\n",
    "    #x_feature  = torch.tensor([getLexicalInputNN(url)] , dtype=torch.float32)\n",
    "    x_feature = torch.tensor([[10.0\t,0\t,0\t,0\t,0\t,0\t,0\t,0\t,0, 1]])\n",
    "    y = model(features = x_feature,input_ids = url_tokenize['input_ids'] , token_type_ids = url_tokenize['token_type_ids'] , attention_mask = url_tokenize['attention_mask']  , labels = torch.tensor([1])).logits\n",
    "    if torch.argmax(y).item() == 0:\n",
    "        return \"Bình thường\"\n",
    "    if torch.argmax(y).item() == 1:\n",
    "        return \"Có tín nhiệm thấp\"\n",
    "\n",
    "def main():\n",
    "    st.sidebar.title('Toxic Website Detector')\n",
    "    app_mode = st.sidebar.selectbox('Menu', ['Toxic Website Detector'])\n",
    "    if(app_mode==\"Toxic Website Detector\"):\n",
    "        st.subheader('Input Single URL')\n",
    "        url_input = st.text_input('Enter URL:')\n",
    "        if st.button('Detect'):\n",
    "            result = detect_toxic_website(url_input)\n",
    "            st.markdown(f'<span style=\"color: yellow; font-size: 20px;\"> Result: {result}</span>', unsafe_allow_html=True)\n",
    "            #st.write(result)  \n",
    "        st.subheader('Input URLs from Excel File')\n",
    "        excel_file = st.file_uploader('Upload Excel file', type=['xlsx'])\n",
    "        if excel_file is not None:\n",
    "            df = pd.read_excel(excel_file)\n",
    "            if st.button('Detect file'):\n",
    "                results = []\n",
    "                for row in df.iterrows():\n",
    "                    result = detect_toxic_website(row[1]['url'])\n",
    "                    results.append(result)\n",
    "                # result_df = pd.DataFrame(results)\n",
    "                # st.write('Results:')\n",
    "                # st.write(result_df)\n",
    "                df['Result'] = results\n",
    "                st.write('Results:')\n",
    "                st.write(df[['url', 'Result']])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
